{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "113f0522-7251-4ed8-9e2f-867f4d5462f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as tr\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import distributions\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59db1f09-f802-4373-96a3-e991422b9cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "if tr.backends.mps.is_available():\n",
    "    device = tr.device(\"mps\")\n",
    "else:\n",
    "    print (\"MPS device not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "430047d7-d370-45cd-ae12-75bb34548dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mass = -0.2\n",
    "lam = 0.5\n",
    "Nd =2\n",
    "mtil = mass +2 *Nd \n",
    "def action(phi):\n",
    "        A = 0.5*mtil*tr.einsum('bxy,bxy->b',phi,phi) + (lam/24.0)*tr.einsum('bxy,bxy->b',phi**2,phi**2)\n",
    "        for mu in range(1,Nd+1):\n",
    "            A = A - tr.einsum('bxy,bxy->b',phi,tr.roll(phi,shifts=-1,dims=mu))\n",
    "        return A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ef8cfa3-3c6a-441f-ae5e-210c67618e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RealNVP(nn.Module):\n",
    "    def __init__(self, nets, nett, mask, prior):\n",
    "        super(RealNVP, self).__init__()\n",
    "        self.prior = prior\n",
    "        self.mask = nn.Parameter(mask, requires_grad=False)\n",
    "        self.t = tr.nn.ModuleList([nett() for _ in range(len(masks))])\n",
    "        self.s = tr.nn.ModuleList([nets() for _ in range(len(masks))])\n",
    "    \n",
    "    # this is the forward start from noise target\n",
    "    def g(self, z):\n",
    "        x = z\n",
    "        for i in range(len(self.t)):\n",
    "            x_ = x*self.mask[i]\n",
    "            s = self.s[i](x_)*(1 - self.mask[i])\n",
    "            t = self.t[i](x_)*(1 - self.mask[i])\n",
    "            x = x_ + (1 - self.mask[i]) * (x * tr.exp(s) + t)\n",
    "        return x\n",
    "    \n",
    "    # this is backward from target to noise\n",
    "    def f(self, x):\n",
    "        log_det_J, z = x.new_zeros(x.shape[0]), x\n",
    "        for i in reversed(range(len(self.t))):\n",
    "            z_ = self.mask[i] * z\n",
    "            s = self.s[i](z_) * (1-self.mask[i])\n",
    "            t = self.t[i](z_) * (1-self.mask[i])\n",
    "            z = (1 - self.mask[i]) * (z - t) * tr.exp(-s) + z_\n",
    "            log_det_J -= s.sum(dim=1)\n",
    "        return z, log_det_J\n",
    "    \n",
    "    def log_prob(self,x):\n",
    "        z, logp = self.f(x)\n",
    "        return self.prior.log_prob(z) + logp #+ self.C\n",
    "        \n",
    "    def sample(self, batchSize): \n",
    "        z = self.prior.sample((batchSize, 1))\n",
    "        #logp = self.prior.log_prob(z)\n",
    "        x = self.g(z)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9ac163a-b18f-40fb-bb31-138657440ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "L=8 # the length of the lattice which is going to be L x L torus\n",
    "V=L*L # the volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aefe5de4-a641-46c7-a445-2494c2e3ff27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 1, 0, 1, 0, 1],\n",
       "       [1, 0, 1, 0, 1, 0, 1, 0],\n",
       "       [0, 1, 0, 1, 0, 1, 0, 1],\n",
       "       [1, 0, 1, 0, 1, 0, 1, 0],\n",
       "       [0, 1, 0, 1, 0, 1, 0, 1],\n",
       "       [1, 0, 1, 0, 1, 0, 1, 0],\n",
       "       [0, 1, 0, 1, 0, 1, 0, 1],\n",
       "       [1, 0, 1, 0, 1, 0, 1, 0]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array(np.arange(L))[:,np.newaxis]\n",
    "Y = np.array(np.arange(L))[np.newaxis,:]\n",
    "#X = X[:,np.newaxis]\n",
    "\n",
    "X = np.repeat(X,L,axis=1)\n",
    "Y = np.repeat(Y,L,axis=0)\n",
    "mm = (X+Y)%2\n",
    "mm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30e79661-d57b-464f-8d93-0cb4d0471be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = mm.reshape(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7156cb30-9d38-4189-ae7b-4ad0d42733b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-83.9537, -89.2414, -88.3813, -90.8288, -93.3673, -89.3514, -88.1707,\n",
       "        -87.8941, -85.0303, -86.6884], device='mps:0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt = distributions.Normal(tr.zeros(V).to(device),tr.ones(V).to(device))\n",
    "prior= distributions.Independent(tt, 1)\n",
    "z = prior.sample((10,1)).squeeze()\n",
    "z.shape\n",
    "prior.log_prob(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4bfab531-a92a-4146-8da5-162011858da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this are functions returning nets\n",
    "#nets = lambda: nn.Sequential(nn.Linear(V, 256), nn.LeakyReLU(), nn.Linear(256, 256), nn.LeakyReLU(), nn.Linear(256, V), nn.Tanh())\n",
    "#nett = lambda: nn.Sequential(nn.Linear(V, 256), nn.LeakyReLU(), nn.Linear(256, 256), nn.LeakyReLU(), nn.Linear(256, V))\n",
    "nets = lambda: nn.Sequential(nn.Linear(V, 2*V), nn.LeakyReLU(), nn.Linear(2*V, V), nn.Tanh())\n",
    "nett = lambda: nn.Sequential(nn.Linear(V, 2*V), nn.LeakyReLU(), nn.Linear(2*V, V))\n",
    "\n",
    "\n",
    "# the number of masks determines layers\n",
    "masks = tr.from_numpy(np.array([lm, 1-lm] * 3).astype(np.float32))\n",
    "normal = distributions.Normal(tr.zeros(V,device=device),tr.ones(V,device=device))\n",
    "prior= distributions.Independent(normal, 1)\n",
    "flow = RealNVP(nets, nett, masks, prior)\n",
    "flow =flow.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a97c5f-0fa4-4f54-940c-7e0192e1307f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/65/q48457md7gn950jfcl5ddv840000gr/T/ipykernel_14974/2740643630.py:8: UserWarning: The operator 'aten::roll' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/mps/MPSFallback.mm:11.)\n",
      "  A = A - tr.einsum('bxy,bxy->b',phi,tr.roll(phi,shifts=-1,dims=mu))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0: loss = 77.825 time = 0.194 seconds\n",
      "iter 10: loss = 53.114 time = 0.472 seconds\n",
      "iter 20: loss = 34.328 time = 0.446 seconds\n",
      "iter 30: loss = 22.750 time = 0.451 seconds\n",
      "iter 40: loss = 14.825 time = 0.445 seconds\n",
      "iter 50: loss = 8.634 time = 0.447 seconds\n",
      "iter 60: loss = 4.280 time = 0.470 seconds\n",
      "iter 70: loss = 0.529 time = 0.456 seconds\n",
      "iter 80: loss = -2.431 time = 0.444 seconds\n",
      "iter 90: loss = -5.033 time = 0.445 seconds\n",
      "iter 100: loss = -6.800 time = 0.444 seconds\n",
      "iter 110: loss = -8.632 time = 0.443 seconds\n",
      "iter 120: loss = -9.974 time = 0.440 seconds\n",
      "iter 130: loss = -11.084 time = 0.444 seconds\n",
      "iter 140: loss = -12.048 time = 0.454 seconds\n",
      "iter 150: loss = -12.965 time = 0.446 seconds\n",
      "iter 160: loss = -13.554 time = 0.451 seconds\n",
      "iter 170: loss = -14.314 time = 0.454 seconds\n",
      "iter 180: loss = -14.910 time = 0.484 seconds\n",
      "iter 190: loss = -15.258 time = 0.449 seconds\n",
      "iter 200: loss = -15.885 time = 0.456 seconds\n",
      "iter 210: loss = -16.098 time = 0.464 seconds\n",
      "iter 220: loss = -16.481 time = 0.447 seconds\n",
      "iter 230: loss = -16.810 time = 0.464 seconds\n",
      "iter 240: loss = -17.124 time = 0.474 seconds\n",
      "iter 250: loss = -17.390 time = 0.471 seconds\n",
      "iter 260: loss = -17.504 time = 0.465 seconds\n",
      "iter 270: loss = -17.752 time = 0.470 seconds\n",
      "iter 280: loss = -17.858 time = 0.466 seconds\n",
      "iter 290: loss = -17.960 time = 0.477 seconds\n",
      "iter 300: loss = -18.229 time = 0.473 seconds\n",
      "iter 310: loss = -18.400 time = 0.480 seconds\n",
      "iter 320: loss = -18.471 time = 0.476 seconds\n",
      "iter 330: loss = -18.468 time = 0.477 seconds\n",
      "iter 340: loss = -18.673 time = 0.468 seconds\n",
      "iter 350: loss = -18.716 time = 0.507 seconds\n",
      "iter 360: loss = -18.918 time = 0.473 seconds\n",
      "iter 370: loss = -18.986 time = 0.471 seconds\n",
      "iter 380: loss = -19.103 time = 0.472 seconds\n",
      "iter 390: loss = -19.222 time = 0.477 seconds\n",
      "iter 400: loss = -19.299 time = 0.489 seconds\n",
      "iter 410: loss = -19.398 time = 0.502 seconds\n",
      "iter 420: loss = -19.432 time = 0.487 seconds\n",
      "iter 430: loss = -19.672 time = 0.470 seconds\n",
      "iter 440: loss = -19.680 time = 0.475 seconds\n",
      "iter 450: loss = -19.797 time = 0.477 seconds\n",
      "iter 460: loss = -19.959 time = 0.474 seconds\n",
      "iter 470: loss = -20.081 time = 0.476 seconds\n",
      "iter 480: loss = -20.263 time = 0.469 seconds\n",
      "iter 490: loss = -20.389 time = 0.479 seconds\n",
      "iter 500: loss = -20.619 time = 0.468 seconds\n",
      "iter 510: loss = -20.859 time = 0.519 seconds\n",
      "iter 520: loss = -21.113 time = 0.483 seconds\n",
      "iter 530: loss = -21.487 time = 0.476 seconds\n",
      "iter 540: loss = -21.804 time = 0.474 seconds\n",
      "iter 550: loss = -22.014 time = 0.467 seconds\n",
      "iter 560: loss = -22.177 time = 0.476 seconds\n",
      "iter 570: loss = -22.233 time = 0.468 seconds\n",
      "iter 580: loss = -22.370 time = 0.467 seconds\n",
      "iter 590: loss = -22.430 time = 0.474 seconds\n",
      "iter 600: loss = -22.478 time = 0.468 seconds\n",
      "iter 610: loss = -22.642 time = 0.472 seconds\n",
      "iter 620: loss = -22.696 time = 0.467 seconds\n",
      "iter 630: loss = -22.771 time = 0.486 seconds\n",
      "iter 640: loss = -22.772 time = 0.467 seconds\n",
      "iter 650: loss = -22.825 time = 0.470 seconds\n",
      "iter 660: loss = -22.919 time = 0.489 seconds\n",
      "iter 670: loss = -22.941 time = 0.501 seconds\n",
      "iter 680: loss = -22.982 time = 0.525 seconds\n",
      "iter 690: loss = -22.995 time = 0.500 seconds\n",
      "iter 700: loss = -23.042 time = 0.507 seconds\n",
      "iter 710: loss = -23.072 time = 0.495 seconds\n",
      "iter 720: loss = -23.089 time = 0.515 seconds\n",
      "iter 730: loss = -23.153 time = 0.492 seconds\n",
      "iter 740: loss = -23.189 time = 0.481 seconds\n",
      "iter 750: loss = -23.200 time = 0.491 seconds\n",
      "iter 760: loss = -23.203 time = 0.478 seconds\n",
      "iter 770: loss = -23.252 time = 0.477 seconds\n",
      "iter 780: loss = -23.249 time = 0.471 seconds\n",
      "iter 790: loss = -23.298 time = 0.478 seconds\n",
      "iter 800: loss = -23.289 time = 0.480 seconds\n",
      "iter 810: loss = -23.367 time = 0.476 seconds\n",
      "iter 820: loss = -23.353 time = 0.477 seconds\n",
      "iter 830: loss = -23.383 time = 0.489 seconds\n",
      "iter 840: loss = -23.405 time = 0.477 seconds\n",
      "iter 850: loss = -23.440 time = 0.493 seconds\n",
      "iter 860: loss = -23.420 time = 0.477 seconds\n",
      "iter 870: loss = -23.478 time = 0.479 seconds\n",
      "iter 880: loss = -23.485 time = 0.475 seconds\n",
      "iter 890: loss = -23.485 time = 0.471 seconds\n",
      "iter 900: loss = -23.500 time = 0.472 seconds\n",
      "iter 910: loss = -23.555 time = 0.473 seconds\n",
      "iter 920: loss = -23.549 time = 0.498 seconds\n",
      "iter 930: loss = -23.508 time = 0.488 seconds\n",
      "iter 940: loss = -23.523 time = 0.466 seconds\n",
      "iter 950: loss = -23.565 time = 0.473 seconds\n",
      "iter 960: loss = -23.574 time = 0.470 seconds\n",
      "iter 970: loss = -23.603 time = 0.465 seconds\n",
      "iter 980: loss = -23.566 time = 0.469 seconds\n",
      "iter 990: loss = -23.626 time = 0.475 seconds\n",
      "iter 1000: loss = -23.607 time = 0.475 seconds\n",
      "iter 1010: loss = -23.629 time = 0.496 seconds\n",
      "iter 1020: loss = -23.648 time = 0.472 seconds\n",
      "iter 1030: loss = -23.623 time = 0.475 seconds\n",
      "iter 1040: loss = -23.674 time = 0.485 seconds\n",
      "iter 1050: loss = -23.662 time = 0.476 seconds\n",
      "iter 1060: loss = -23.729 time = 0.475 seconds\n",
      "iter 1070: loss = -23.719 time = 0.472 seconds\n",
      "iter 1080: loss = -23.675 time = 0.467 seconds\n",
      "iter 1090: loss = -23.730 time = 0.477 seconds\n",
      "iter 1100: loss = -23.665 time = 0.480 seconds\n",
      "iter 1110: loss = -23.743 time = 0.475 seconds\n",
      "iter 1120: loss = -23.752 time = 0.480 seconds\n",
      "iter 1130: loss = -23.752 time = 0.474 seconds\n",
      "iter 1140: loss = -23.763 time = 0.478 seconds\n",
      "iter 1150: loss = -23.762 time = 0.472 seconds\n",
      "iter 1160: loss = -23.814 time = 0.474 seconds\n",
      "iter 1170: loss = -23.803 time = 0.467 seconds\n",
      "iter 1180: loss = -23.786 time = 0.469 seconds\n",
      "iter 1190: loss = -23.796 time = 0.469 seconds\n",
      "iter 1200: loss = -23.851 time = 0.483 seconds\n",
      "iter 1210: loss = -23.841 time = 0.480 seconds\n",
      "iter 1220: loss = -23.828 time = 0.482 seconds\n",
      "iter 1230: loss = -23.812 time = 0.474 seconds\n",
      "iter 1240: loss = -23.871 time = 0.487 seconds\n",
      "iter 1250: loss = -23.894 time = 0.483 seconds\n",
      "iter 1260: loss = -23.903 time = 0.492 seconds\n",
      "iter 1270: loss = -23.882 time = 0.501 seconds\n",
      "iter 1280: loss = -23.854 time = 0.481 seconds\n",
      "iter 1290: loss = -23.860 time = 0.504 seconds\n",
      "iter 1300: loss = -23.904 time = 0.469 seconds\n",
      "iter 1310: loss = -23.932 time = 0.484 seconds\n",
      "iter 1320: loss = -23.942 time = 0.477 seconds\n",
      "iter 1330: loss = -23.929 time = 0.473 seconds\n",
      "iter 1340: loss = -23.945 time = 0.471 seconds\n",
      "iter 1350: loss = -23.955 time = 0.479 seconds\n",
      "iter 1360: loss = -23.965 time = 0.478 seconds\n",
      "iter 1370: loss = -23.973 time = 0.479 seconds\n",
      "iter 1380: loss = -23.969 time = 0.488 seconds\n",
      "iter 1390: loss = -23.967 time = 0.481 seconds\n",
      "iter 1400: loss = -24.001 time = 0.479 seconds\n",
      "iter 1410: loss = -24.024 time = 0.484 seconds\n",
      "iter 1420: loss = -24.012 time = 0.483 seconds\n",
      "iter 1430: loss = -24.009 time = 0.476 seconds\n",
      "iter 1440: loss = -24.018 time = 0.474 seconds\n",
      "iter 1450: loss = -24.051 time = 0.509 seconds\n",
      "iter 1460: loss = -24.034 time = 0.486 seconds\n",
      "iter 1470: loss = -24.054 time = 0.486 seconds\n",
      "iter 1480: loss = -24.061 time = 0.484 seconds\n",
      "iter 1490: loss = -24.026 time = 0.478 seconds\n",
      "iter 1500: loss = -24.085 time = 0.479 seconds\n",
      "iter 1510: loss = -24.089 time = 0.470 seconds\n",
      "iter 1520: loss = -24.080 time = 0.474 seconds\n",
      "iter 1530: loss = -24.075 time = 0.473 seconds\n",
      "iter 1540: loss = -24.100 time = 0.453 seconds\n",
      "iter 1550: loss = -24.144 time = 0.445 seconds\n",
      "iter 1560: loss = -24.096 time = 0.443 seconds\n",
      "iter 1570: loss = -24.118 time = 0.480 seconds\n",
      "iter 1580: loss = -24.111 time = 0.479 seconds\n",
      "iter 1590: loss = -24.112 time = 0.440 seconds\n",
      "iter 1600: loss = -24.126 time = 0.441 seconds\n",
      "iter 1610: loss = -24.165 time = 0.448 seconds\n",
      "iter 1620: loss = -24.137 time = 0.483 seconds\n",
      "iter 1630: loss = -24.130 time = 0.447 seconds\n",
      "iter 1640: loss = -24.183 time = 0.448 seconds\n",
      "iter 1650: loss = -24.205 time = 0.446 seconds\n",
      "iter 1660: loss = -24.165 time = 0.444 seconds\n",
      "iter 1670: loss = -24.159 time = 0.459 seconds\n",
      "iter 1680: loss = -24.192 time = 0.502 seconds\n",
      "iter 1690: loss = -24.209 time = 0.483 seconds\n",
      "iter 1700: loss = -24.179 time = 0.456 seconds\n",
      "iter 1710: loss = -24.199 time = 0.444 seconds\n",
      "iter 1720: loss = -24.190 time = 0.473 seconds\n",
      "iter 1730: loss = -24.209 time = 0.479 seconds\n",
      "iter 1740: loss = -24.183 time = 0.454 seconds\n",
      "iter 1750: loss = -24.238 time = 0.449 seconds\n",
      "iter 1760: loss = -24.206 time = 0.445 seconds\n",
      "iter 1770: loss = -24.211 time = 0.446 seconds\n",
      "iter 1780: loss = -24.206 time = 0.482 seconds\n",
      "iter 1790: loss = -24.222 time = 0.468 seconds\n",
      "iter 1800: loss = -24.269 time = 0.472 seconds\n",
      "iter 1810: loss = -24.261 time = 0.449 seconds\n",
      "iter 1820: loss = -24.273 time = 0.449 seconds\n",
      "iter 1830: loss = -24.255 time = 0.455 seconds\n",
      "iter 1840: loss = -24.267 time = 0.463 seconds\n",
      "iter 1850: loss = -24.274 time = 0.487 seconds\n",
      "iter 1860: loss = -24.257 time = 0.458 seconds\n",
      "iter 1870: loss = -24.282 time = 0.452 seconds\n",
      "iter 1880: loss = -24.260 time = 0.442 seconds\n",
      "iter 1890: loss = -24.254 time = 0.474 seconds\n",
      "iter 1900: loss = -24.304 time = 0.452 seconds\n",
      "iter 1910: loss = -24.335 time = 0.460 seconds\n",
      "iter 1920: loss = -24.305 time = 0.662 seconds\n",
      "iter 1930: loss = -24.308 time = 0.527 seconds\n",
      "iter 1940: loss = -24.322 time = 0.554 seconds\n",
      "iter 1950: loss = -24.324 time = 0.551 seconds\n",
      "iter 1960: loss = -24.321 time = 0.577 seconds\n",
      "iter 1970: loss = -24.323 time = 0.556 seconds\n",
      "iter 1980: loss = -24.306 time = 0.549 seconds\n",
      "iter 1990: loss = -24.299 time = 0.549 seconds\n",
      "iter 2000: loss = -24.317 time = 0.553 seconds\n",
      "iter 2010: loss = -24.365 time = 0.537 seconds\n",
      "iter 2020: loss = -24.381 time = 0.553 seconds\n",
      "iter 2030: loss = -24.371 time = 0.554 seconds\n",
      "iter 2040: loss = -24.367 time = 0.549 seconds\n",
      "iter 2050: loss = -24.368 time = 0.543 seconds\n",
      "iter 2060: loss = -24.398 time = 0.542 seconds\n",
      "iter 2070: loss = -24.394 time = 0.545 seconds\n",
      "iter 2080: loss = -24.347 time = 0.544 seconds\n",
      "iter 2090: loss = -24.378 time = 0.545 seconds\n",
      "iter 2100: loss = -24.378 time = 0.546 seconds\n",
      "iter 2110: loss = -24.393 time = 0.570 seconds\n",
      "iter 2120: loss = -24.387 time = 0.548 seconds\n",
      "iter 2130: loss = -24.395 time = 0.543 seconds\n",
      "iter 2140: loss = -24.398 time = 0.546 seconds\n",
      "iter 2150: loss = -24.400 time = 0.544 seconds\n",
      "iter 2160: loss = -24.421 time = 0.537 seconds\n",
      "iter 2170: loss = -24.384 time = 0.548 seconds\n",
      "iter 2180: loss = -24.434 time = 0.538 seconds\n",
      "iter 2190: loss = -24.440 time = 0.536 seconds\n",
      "iter 2200: loss = -24.431 time = 0.538 seconds\n",
      "iter 2210: loss = -24.409 time = 0.549 seconds\n",
      "iter 2220: loss = -24.437 time = 0.554 seconds\n",
      "iter 2230: loss = -24.402 time = 0.561 seconds\n",
      "iter 2240: loss = -24.420 time = 0.559 seconds\n",
      "iter 2250: loss = -24.463 time = 0.591 seconds\n",
      "iter 2260: loss = -24.436 time = 0.574 seconds\n",
      "iter 2270: loss = -24.422 time = 0.569 seconds\n",
      "iter 2280: loss = -24.455 time = 0.556 seconds\n",
      "iter 2290: loss = -24.437 time = 0.561 seconds\n",
      "iter 2300: loss = -24.485 time = 0.563 seconds\n",
      "iter 2310: loss = -24.464 time = 0.555 seconds\n",
      "iter 2320: loss = -24.503 time = 0.565 seconds\n",
      "iter 2330: loss = -24.494 time = 0.555 seconds\n",
      "iter 2340: loss = -24.476 time = 0.562 seconds\n",
      "iter 2350: loss = -24.478 time = 0.556 seconds\n",
      "iter 2360: loss = -24.473 time = 0.550 seconds\n",
      "iter 2370: loss = -24.500 time = 0.548 seconds\n",
      "iter 2380: loss = -24.451 time = 0.544 seconds\n",
      "iter 2390: loss = -24.507 time = 0.569 seconds\n",
      "iter 2400: loss = -24.505 time = 0.548 seconds\n",
      "iter 2410: loss = -24.476 time = 0.545 seconds\n",
      "iter 2420: loss = -24.491 time = 0.558 seconds\n",
      "iter 2430: loss = -24.483 time = 0.549 seconds\n",
      "iter 2440: loss = -24.525 time = 0.551 seconds\n",
      "iter 2450: loss = -24.502 time = 0.568 seconds\n",
      "iter 2460: loss = -24.516 time = 0.551 seconds\n",
      "iter 2470: loss = -24.488 time = 0.551 seconds\n",
      "iter 2480: loss = -24.517 time = 0.556 seconds\n",
      "iter 2490: loss = -24.542 time = 0.549 seconds\n",
      "iter 2500: loss = -24.543 time = 0.558 seconds\n",
      "iter 2510: loss = -24.515 time = 0.549 seconds\n",
      "iter 2520: loss = -24.544 time = 0.552 seconds\n",
      "iter 2530: loss = -24.553 time = 0.549 seconds\n",
      "iter 2540: loss = -24.525 time = 0.578 seconds\n",
      "iter 2550: loss = -24.525 time = 0.555 seconds\n",
      "iter 2560: loss = -24.545 time = 0.555 seconds\n",
      "iter 2570: loss = -24.548 time = 0.549 seconds\n",
      "iter 2580: loss = -24.553 time = 0.557 seconds\n",
      "iter 2590: loss = -24.549 time = 0.547 seconds\n",
      "iter 2600: loss = -24.547 time = 0.546 seconds\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2*2048\n",
    "optimizer = tr.optim.Adam([p for p in flow.parameters() if p.requires_grad==True], lr=1e-4)\n",
    "tic=time.perf_counter()\n",
    "for t in range(5001):   \n",
    "    #with torch.no_grad():\n",
    "    z = prior.sample((batch_size, 1)).squeeze()\n",
    "    x = flow.g(z) # generate a sample\n",
    "    loss = (flow.log_prob(x)+action(x.view(batch_size,L,L))).mean() # KL divergence (or not?)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward(retain_graph=True)\n",
    "    optimizer.step() \n",
    "    if t % 500 == 0:\n",
    "        toc=time.perf_counter()\n",
    "        #print(z.shape)\n",
    "        print('iter %s:' % t, 'loss = %.3f' % loss,'time = %.3f' % (toc-tic),'seconds')\n",
    "        tic=time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc34970-6abe-4dad-9169-1c12d2e4649a",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = prior.sample((batch_size, 1)).squeeze()\n",
    "x = flow.g(z)\n",
    "x.shape\n",
    "x,j = flow.f(z)\n",
    "prior.log_prob(z).shape,j.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122ec0ce-1421-403a-8542-b36ba0c128ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = prior.sample((10, 1)).squeeze()\n",
    "x = flow.g(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88411847-c3b4-4e19-bac9-41ed982730d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "zz,j=flow.f(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79333093-f1a4-4c6a-aa56-01bcfffd5cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(zz.requires_grad,z.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f380ba6-7b78-45b3-983b-7249ba5a0d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tr.sum(tr.abs(zz-z))/V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ebef39-138f-43e9-b5c4-f6b10a0e51e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = action(x.view(x.shape[0],L,L))+flow.log_prob(x)\n",
    "print(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a106cd-f3dc-4655-92d1-2d0b2594d465",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(diff - diff.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd05c4c-33f9-40c1-a1c2-c60a9369cfad",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = prior.sample((2000, 1)).squeeze()\n",
    "xz = flow.g(z).detach()\n",
    "diff = action(xz.view(xz.shape[0],L,L))+flow.log_prob(xz)\n",
    "diff.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df62867b-1cbf-45e2-885f-c484672d87d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = flow.sample(2000).detach().cpu().numpy()\n",
    "plt.scatter(x[:, 0,0], x[:, 0,1], c='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fc819a-fbcf-4f76-a0e4-647a99b87482",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9565d1-68a6-4897-8bfd-76f0b853bb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_diff = diff.mean()\n",
    "diff -= m_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04541b9-8737-40dd-af8a-3907dfba4176",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"max  action diff: \", tr.max(diff.abs()).cpu().detach().numpy())\n",
    "print(\"min  action diff: \", tr.min(diff.abs()).cpu().detach().numpy())\n",
    "print(\"mean action diff: \", m_diff.detach().cpu().detach().numpy())\n",
    "print(\"std  action diff: \", diff.std().cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73beb8d6-10e3-4639-92c4-63e074ca38b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "foo = tr.exp(-diff)\n",
    "#print(foo)\n",
    "w = foo/tr.mean(foo)\n",
    "\n",
    "print(\"mean re-weighting factor: \" , w.mean().cpu().detach().numpy())\n",
    "print(\"std  re-weighting factor: \" , w.std().cpu().detach().numpy())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a48529b-ff0f-48c6-ab1e-375496b9d26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "logbins = np.logspace(np.log10(5e-2),np.log10(5e1),int(w.shape[0]/10))\n",
    "plt.hist(w.detach().cpu(),bins=logbins)\n",
    "plt.xscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b633cc-ed25-4032-b204-28a5ac5748d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "c=0\n",
    "for tt in flow.parameters():\n",
    "    #print(tt.shape)\n",
    "    if tt.requires_grad==True :\n",
    "        c+=tt.numel()\n",
    "        \n",
    "print(\"parameter count: \",c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9845c649-2ec2-4ea6-a1de-7f2b636afdaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e9ba77-8aaf-48f7-a665-fd399950d8ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
